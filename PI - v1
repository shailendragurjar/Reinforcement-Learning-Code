#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Tue Aug 29 21:32:19 2017

@author: shailendragurjar
"""
import numpy as np

states = 44
actions = 2
gamma = 0.99

np.random.seed(22)
T = np.reshape([np.random.sample() for i in range(states*actions*states)], [states, actions, states]) 
R = np.reshape([np.random.uniform(-1,1,states*actions*states)],[states, actions, states])
T = T/np.expand_dims(np.sum(T, axis=2), axis=2)

# initialize policy and value arbitrarily
#policy = [0 for s in range(states)]
policy = np.random.randint(2, size=states)
old_pol = list(policy)
V = np.zeros(states)

# PI Howard
print "Initial policy", policy
# print V
# print P
# print R


is_value_changed = True
iterations = 0
while is_value_changed:
    is_value_changed = False
    iterations += 1
    print "Iterations:", iterations
    # run value iteration for each state
    for s in range(states):
        val = V[s]
        V[s] = sum([T[s,policy[s],s1] * (R[s,policy[s],s1] + gamma*V[s1]) for s1 in range(states)])
        # print "Run for state", s        
    print V
    for s in range(states):
        q_best = V[s]
        # print "State", s, "q_best", q_best
        for a in range(actions):
            q_sa = sum([T[s, a, s1] * (R[s, a, s1] + gamma * V[s1]) for s1 in range(states)])
            if q_sa > q_best:
                print "State", s, ": q_sa", q_sa, "q_best", q_best
                policy[s] = a
                q_best = q_sa
              #  is_value_changed = True
    print policy
    if (np.equal(old_pol,policy).all()):
        is_value_changed = False
    else:
        old_pol = list(policy)
        is_value_changed = True
        
    print "Policy now", policy
    print "V", V

print "Final policy"
print policy
print V

# LP
from scipy.optimize import linprog as lp
states = 4
actions = 2
gamma = 0.99

np.random.seed(22)
T = np.reshape([np.random.sample() for i in range(states*actions*states)], [states, actions, states]) 
R = np.reshape([np.random.uniform(-1,1,states*actions*states)],[states, actions, states])
T = T/np.expand_dims(np.sum(T, axis=2), axis=2)

# initialize policy and value arbitrarily
policy = [0 for s in range(states)]

x1, x2 = [], []
c =np.ones(states,dtype=int)

for i in range(states):
    for a in range(actions):
        coef_x1, coef_x2 = [0]*states, 0
        for j in range(states):
            coef_x1[j] = T[i][a][j]*gamma
            coef_x2 += T[i][a][j]*R[i][a][j]
            coef_x1[i] -= 1
            x1.append(coef_x1)
            x2.append(coef_x2)
    x2_new = [i*-1 for i in x2]
    res = lp(c, x1, x2_new)
    V_LP = res.x
            
for s in range(states):
    q_best = V_LP[s]
    # print "State", s, "q_best", q_best
    for a in range(actions):
        q_sa = sum([T[s, a, s1] * (R[s, a, s1] + gamma * V_LP[s1]) for s1 in range(states)])
        if q_sa > q_best:
            print "State", s, ": q_sa", q_sa, "q_best", q_best
            policy[s] = a
            q_best = q_sa
             #   is_value_changed = True
    print policy
    print "Policy now", policy

print "Final policy"
print policy
print V_LP

# PI Mansour and Singh
